Algorithms

	- the link between input & output
	- Correctness
	- Design
		- efficient (not too much memory, not too long)
			- How long do algorithms run (time)
		- well-structured, easy to read
	

Big O notation
	- How long algorithm takes to run
	- "Order" of time execution
	- upper bound (worst case scenario)
	- always takes the worst case components
	- typically used to consider efficiency of algorithms
	
	O(n^2) : Exponential
	O(n) : Linear
	O(Log2(n)) : Log
	O(1) : Constant (as n increases, steps still same)
	
	
Omega
	- How few steps will algorithm take
	- lower bound (best case scenario)
	
Theta
	- Upper bound = lower bound




Searching
	- finding info within data
	- Typically within array
	
	
	Linear search
		- correct, but may not be efficient
		- good if unsure what is values in array
		- O(n) search time (worst case, item at the end of array)
		- Omega(1) search time (right at the spot where search began) (best case scenario)
		
	
	Binary Search
		- Array must be sorted
		- Start in middle (divide & conquer) & continue until item found
		- O(log n) : Worst case scenario
		- Omega(1) : Best case scenario (desired right at the middle)
		
	

Data Structure (C)

	- something like class
	- create own data type
	- Person datatype
	- each datatype have own attributes
	
	typedef struct {
	
		string name;
		string mobile;
		
	}
	Person;
	
	Person people[2];
	
	people[0].name = "Lucas";
	people[0].mobile = "98765432";
	
	
Sorting Algorithms
	
	1) Bubble sort: O(n^2), Omega(n) : if already sorted and no swap done, only n moves
		- highest number will bubble towards the end of array
		- compare side by side elements, higher number move rightwards
		- (n-1) outer loop * (n-1) inner loop = n^2 -2n + 1
		- omega(n) only have if no swap condition, else omega(n^2)
	
	2) Selection sort: O(n(n+1)/2) = O(n^2), Omega(n^2) : Already sorted but still run thru all
		- find smallest element
		- swap smallest element with item at first index
			- if smallest already there, no swap needed
		- continue until all are sorted
	
	
	n^2 will cause elements to be re-analyzed per loop
	- first 2 are known as comparison sorts
	
	
	
Recursion
	- Function calling itself
	
	- need base case and recursive case
		- base case: condition to stop recursion
		- recursive case: function calling itself
	
	
	void draw(int n) {
		for (int i = 0; i < n; i++) {
			for (int j = 0; j < i + 1; j++) {
				printf("#");
			}
		}
	}
	
	
	Recursion implementation:
	
	void draw(int n) {
		
		// base case
		if (n <= 0){
			return;
		}
		
		// recursive case
		draw(n-1)

		for(int i = 0; i < n; i++) {
			printf("#");
		}
	}
	
	3) Merge Sort: O(n log n), omega(n log n) <- worse than bubble for omega
		- call upon itself to perform sorting
		- sort left half & sort right half then merge together at the end
		- uses a bit more memory (requires array to store left & right half)
		- but faster O
		
		if only 1 number,
			quit
		
		1) Find the middle of array (the half) & split into left half and right half
			sort left half
				- keep calling itself until 1 element, then its sorted
			sort right half
				- keep calling itself until 1 element, then its sorted
		merge sorted halves
			- look at beginning of 2 halves, lower value goes into combined array
			
		eg.
		
		5 2 7 4 1 6 3 0
		
		1) Find middle and split into left half and right half
		5 2 7 4 & 1 6 3 0
		
		2) Sort left half (5 2 7 4)
		
		3) Sort left half (5 2)
		
		4) Sort left half (5) (already sorted)
			- Right half = 2
			
		5) compare left & right half (5 and 2)
			lower value to the left : 2 5
			
		Sort right half:
		4 7  
		
		Sort left and right halves
		
		2 5 and 4 7
		
		2 4 5 7
		
		Do the same for 1 6 3 0 -> 0 1 3 6
		
		Then sort left half (2457) and right half (0136)
		0 1 2 3 4 5 6 7
		 
		
			